# Лабораторные работы №6-8

------------------------------------------------
|  Выполнил    |    Группа       | Дата        |
|--------------|-----------------|-------------|
| Карнаков Н.Д.|    М8О-406Б-21  |   10.05.2025|

## Лабораторная работа №6 (Проведение исследований с моделями классификации)

### Сравнительная таблица 

| Model                              | Accuracy  | Precision | Recall   | F1-score | ROC-AUC  | PR-AUC   |
| ---------------------------------- | --------- | --------- | -------- | -------- | -------- | -------- |
| ResNet18 (baseline)                | 0.782051  | 0.746124  | 0.987179 | 0.849890 | 0.944587 | 0.959907 |
| ViT-B_16 (baseline)                | 0.727564  | 0.701465  | 0.982051 | 0.818376 | 0.932807 | 0.958099 |
| ResNet18 (improved)                | 0.931093  | 0.914081  | 0.982051 | 0.946848 | 0.975093 | 0.983458 |
| ViT-B_16 (improved)                | 0.868590  | 0.955621  | 0.828205 | 0.887363 | 0.960618 | 0.973207 |
| CustomCNN (baseline)               | 0.716346  | 0.689840  | 0.992308 | 0.813880 | 0.843809 | 0.854794 |
| CustomCNN (improved)               | 0.504808  | 0.987952  | 0.210256 | 0.346723 | 0.898619 | 0.933004 |
| Custom ViT (baseline)              | 0.625000  | 0.625000  | 1.000000 | 0.769231 | 0.500000 | 0.625000 |
| Custom ViT (improved)              | 0.625000  | 0.625000  | 1.000000 | 0.769231 | 0.500000 | 0.625000 |

На основе всестороннего эксперимента с восемью моделями для задачи бинарной классификации рентгеновских снимков лёгких была выявлена чёткая зависимость между предобученной архитектурой и объёмом обучающей выборки, а также эффективностью прикладных приёмов улучшения качества.

Во-первых, модели ResNet-18 и ViT-B_16, предобученные на ImageNet и использованные в качестве бейзлайна, продемонстрировали высокую базовую производительность: ResNet-18 показала сбалансированные метрики (accuracy ≈ 78 %, recall ≈ 99 %, precision ≈ 75 %, F1 ≈ 85 %), а ViT-B_16 чуть уступала ей по F1 (≈ 82 %), но уже обладала преимущественно высокой способностью к разделению классов (ROC-AUC ≈ 93 %). Применение комплекса улучшений — расширенные аугментации, взвешенная функция потерь и управление скоростью обучения через LR-scheduler — позволило ResNet-18 достичь accuracy ≈ 93 %, recall ≈ 98 % и precision ≈ 91 % (F1 ≈ 95 %), тогда как ViT-B_16 (improved) повысил precision до ≈ 96 % при F1 ≈ 89 %. Данные результаты подчёркивают, что предобученные на больших корпусах изображения CNN и трансформеры наиболее адекватно адаптируются к медицинским изображениям после донастройки.

Во-вторых, самостоятельная разработка сверточной нейронной сети (CustomCNN) с четырьмя блоками и двумя полносвязными слоями, обученная «с нуля» на том же ограниченном объёме данных, продемонстрировала высокий recall (≈ 99 %) за счёт массовых ложных срабатываний (precision ≈ 69 %, F1 ≈ 81 %), что свидетельствует о её склонности к максимизации полноты при минимальных объёмах. Попытка усилить модель аналогичным комплексом улучшений привела к обратному эффекту: precision возрос до ≈ 99 %, а recall упал до ≈ 21 %, что говорит о чрезмерной осторожности классификатора при отсутствии достаточной обучающей выборки.

В-третьих, собственная реализация Vision Transformer также показала крайне ограниченные возможности при обучении «с нуля» на 10 % данных: базовый Custom ViT практически всегда предсказывал один класс (accuracy ≈ 62 %, ROC-AUC = 0.5), а внесённые улучшения оказались бессильны сместить эту стагнацию. Это подчёркивает важность предобучения трансформеров на масштабных наборах данных и необходимости существенно большего объёма разнообразных медицинских изображений для их успешной адаптации.

Таким образом, результаты всего исследования позволяют заключить, что для задач классификации медицинских изображений при ограниченном объёме аннотированных данных оптимальным выбором являются предобученные архитектуры (ResNet, ViT) с дальнейшей донастройкой на целевом домене с применением техники аугментаций, балансировки классов и адаптивного изменения learning rate. Собственные модели без предобучения демонстрируют недостаточную способность к генерализации или, напротив, слишком радикальные компромиссы между recall и precision. Для дальнейшего улучшения качества классификации рекомендуется расширять аннотированные датасеты, использовать ансамбли предобученных архитектур и внедрять пороговую оптимизацию для достижения необходимого баланса между ошибками первого и второго рода.
